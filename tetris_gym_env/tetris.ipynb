{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: ###\n",
    "- Implement additional bonus score features:\n",
    "    - Full clear\n",
    "    - Combos\n",
    "- Select NN type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "# import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "# from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "# import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:20: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: float32. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:25: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_id = \"gym_tetris:tetris_rl\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "s_size = env.observation_space.shape\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  (1, 29, 10)\n",
      "Sample observation [[[1.3823761  0.55758595 1.1091311  0.41747603 0.51495945 1.3864182\n",
      "   1.7224386  1.073062   0.44632304 1.8973005 ]\n",
      "  [0.7309322  1.3727808  1.5753471  1.1433415  1.6789643  0.5521697\n",
      "   1.3483944  0.77722937 1.7047476  0.50821936]\n",
      "  [1.0808114  1.3938103  0.01558153 0.165352   1.6665164  0.90415347\n",
      "   0.95690686 1.058503   1.146038   0.43344957]\n",
      "  [0.19206783 0.08784577 0.38814518 0.0799054  0.7228692  1.5988367\n",
      "   1.1009266  0.6234679  1.288257   0.42813107]\n",
      "  [0.8626415  1.7106012  1.5913675  1.6740184  1.5093381  1.8657327\n",
      "   0.25874957 1.9217572  0.2686025  0.60886234]\n",
      "  [0.9345636  0.6329479  0.81135106 0.87246037 1.9168451  0.2734837\n",
      "   0.8348335  1.439792   1.0792379  1.2434831 ]\n",
      "  [1.5764691  1.563034   1.4931544  0.02460025 1.1581335  0.6180945\n",
      "   1.9023187  1.9692788  0.08114924 0.16439523]\n",
      "  [1.9224085  1.2069038  0.19290449 1.7726233  1.0573136  0.5971366\n",
      "   1.9962649  0.49996603 1.431833   1.8353467 ]\n",
      "  [1.8708569  1.774254   1.4021419  1.8184937  0.7724679  1.9545271\n",
      "   1.0694896  1.8018212  0.5072907  1.1345537 ]\n",
      "  [1.5173591  1.2087563  1.413252   0.24127163 0.37890902 0.46162364\n",
      "   0.9376444  0.58392626 1.4851304  0.32569173]\n",
      "  [1.9455047  1.3752811  1.0193235  1.4689494  0.31610548 0.15106924\n",
      "   0.26191846 1.3345952  1.5426283  1.9152848 ]\n",
      "  [0.21054202 1.6820297  1.0577791  1.207128   0.95885664 1.793588\n",
      "   0.20909429 1.7960721  0.9182115  0.5812943 ]\n",
      "  [0.03721967 0.20874576 1.2519326  0.44173792 1.0948985  0.6227404\n",
      "   0.79825556 1.4752373  1.0054507  0.5907741 ]\n",
      "  [1.0280665  1.4398639  0.9812527  0.7247772  0.77128303 0.69057983\n",
      "   0.9519887  1.6491537  0.6883763  1.0964952 ]\n",
      "  [1.7881291  0.13916096 1.2482486  1.3986999  1.4177251  1.5231003\n",
      "   1.2056044  1.8586081  1.6555146  0.7867553 ]\n",
      "  [0.20974766 1.8350254  0.81067    1.1001107  0.17757666 0.310605\n",
      "   1.3691036  0.23532508 0.5453347  1.4527035 ]\n",
      "  [0.889037   1.8372817  0.3076777  0.37146226 0.69098073 0.59317434\n",
      "   1.105361   1.4765673  0.0633349  0.16378641]\n",
      "  [0.25721422 1.4177903  0.6318397  1.8537854  0.05740638 0.90446645\n",
      "   1.8226207  1.4929419  1.876753   1.8922517 ]\n",
      "  [0.10186082 0.5655965  1.6343125  0.28876472 1.5623986  0.83131635\n",
      "   0.07034811 1.4788692  1.2274863  0.1850266 ]\n",
      "  [0.19770636 0.02704188 1.9500735  0.53262174 0.58347756 1.9234388\n",
      "   0.7071758  0.49317157 1.3538262  1.9117035 ]\n",
      "  [1.7535375  0.7737864  0.47542354 0.5733852  1.1941799  0.26467025\n",
      "   0.38316634 1.6366059  0.27947113 1.9307438 ]\n",
      "  [0.43021095 1.4242886  0.93594426 0.52764386 1.4688025  0.05385658\n",
      "   1.2520525  1.6125362  0.56562394 1.5571623 ]\n",
      "  [1.3014708  0.92505723 0.96598375 0.22777292 0.57344264 0.29540426\n",
      "   1.3512441  0.99560857 0.97584134 1.8494146 ]\n",
      "  [0.15637672 1.6782533  0.6909018  1.1388283  0.32291967 0.6842471\n",
      "   0.55012    1.8357687  0.6200796  0.54671025]\n",
      "  [1.0967423  0.55845475 1.978797   0.23239428 0.8436207  1.7313231\n",
      "   1.5332333  1.7269298  0.08346284 0.13118112]\n",
      "  [0.59313905 0.36383963 0.28657538 0.8835299  1.3517584  0.36114034\n",
      "   0.8018324  0.08780398 0.9786276  0.6941628 ]\n",
      "  [0.45779833 1.1594967  0.5599175  1.1660596  1.4852594  1.6011926\n",
      "   1.0034021  0.40353972 0.05907074 1.785344  ]\n",
      "  [1.8827107  0.24165276 0.8842327  0.34973073 0.43375066 0.04508488\n",
      "   0.2240075  1.8128877  1.4318475  0.7681834 ]\n",
      "  [1.5516783  1.5921947  1.9581319  0.91687185 1.6555822  1.6087382\n",
      "   0.48008    1.5009749  0.44469115 0.2662689 ]]]\n",
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  7\n",
      "Action Space Sample 3\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n",
    "\n",
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 29, 10)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size, h2_size):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # Convolutional Network\n",
    "        # self.conv = nn.Sequential(nn.Conv2d(s_size[0], h_size, 4), nn.ReLU(),\n",
    "        #     nn.Conv2d(h_size, h_size, kernel_size=3, stride=1),nn.ReLU())\n",
    "        # # print(self.conv(torch.zeros(1,*s_size,4)).size())\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(int(np.prod(self.conv(torch.zeros(1,*s_size)).size())), h2_size),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(h2_size, a_size)\n",
    "        # )\n",
    "\n",
    "        # Linear Network\n",
    "        self.fc1 = nn.Linear(np.prod(s_size), h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size)\n",
    "        self.fc3 = nn.Linear(h_size, a_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convlutional Network\n",
    "        # x = self.conv(x).view(x.size()[0], -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        # Linear Network\n",
    "        x = np.reshape(x,(1,-1))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # print(np.shape(x))\n",
    "        # We output the softmax\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state) # Code Here: get the action\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "\n",
    "        ## We compute this starting from the last timestep to the first, to avoid redundant computations\n",
    "\n",
    "        ## appendleft() function of queues appends to the position 0\n",
    "        ## We use deque instead of lists to reduce the time complexity\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(gamma*disc_return_t + rewards[t]) # Code Here: complete here\n",
    "\n",
    "        ## standardization for training stability\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps is added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Line 8: PyTorch prefers gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"h2_size\": 512,\n",
    "    \"n_training_episodes\": 10000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "torch.manual_seed(50) # Don't change this\n",
    "tetris_policy = Policy(hyperparameters[\"state_space\"], hyperparameters[\"action_space\"], hyperparameters[\"h_size\"], hyperparameters[\"h2_size\"]).to(device)\n",
    "tetris_optimizer = optim.Adam(tetris_policy.parameters(), lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scores \u001b[39m=\u001b[39m reinforce(tetris_policy,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                    tetris_optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                    hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mn_training_episodes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                    hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mmax_t\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                    hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                    \u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m log_prob, disc_return \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(saved_log_probs, returns):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     policy_loss\u001b[39m.\u001b[39mappend(\u001b[39m-\u001b[39mlog_prob \u001b[39m*\u001b[39m disc_return)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(policy_loss)\u001b[39m.\u001b[39msum()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Line 8: PyTorch prefers gradient descent\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "scores = reinforce(tetris_policy,\n",
    "                   tetris_optimizer,\n",
    "                   hyperparameters[\"n_training_episodes\"],\n",
    "                   hyperparameters[\"max_t\"],\n",
    "                   hyperparameters[\"gamma\"],\n",
    "                   100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
