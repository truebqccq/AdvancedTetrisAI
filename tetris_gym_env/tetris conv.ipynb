{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "# import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "# from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Tetris v2.0\n",
      "Build Tetris v2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:20: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: float32. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:25: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_id = \"gym_tetris:tetris_rl\"\n",
    "env = gym.make(env_id, render_mode='none')\n",
    "eval_env = gym.make(env_id, render_mode='rgb_array')\n",
    "s_size = env.observation_space.shape\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  (1, 29, 10)\n",
      "Sample observation [[[0.5001674  1.9444618  0.12471315 1.3674952  1.4446001  1.2329413\n",
      "   0.80900884 1.471096   0.3056068  1.4130958 ]\n",
      "  [0.92113864 0.16315493 1.1360261  0.06658313 0.21662323 0.3295408\n",
      "   0.73913896 1.6171783  1.9990059  0.7195149 ]\n",
      "  [0.55860716 1.9216217  0.5916439  1.224774   1.0354251  0.01833664\n",
      "   1.2239193  1.3778841  1.7783587  0.52304804]\n",
      "  [1.9775467  0.6524452  1.1320696  1.3931639  1.577205   1.4921728\n",
      "   0.11724084 0.12208962 1.9882985  0.22672121]\n",
      "  [0.80771726 0.8356227  1.7874684  0.28889617 0.88067275 1.5303949\n",
      "   1.1525766  1.7875986  1.4853914  1.817581  ]\n",
      "  [1.1156816  0.54448175 1.3786387  1.3334324  0.14255355 1.9888345\n",
      "   0.40824574 1.9945737  0.64459604 0.97752464]\n",
      "  [1.3538848  0.09452998 1.2625912  1.9983845  0.6797982  1.8307283\n",
      "   0.20424323 1.9423357  0.74914616 0.12598668]\n",
      "  [1.0699419  0.6570036  1.9892824  1.9916095  0.9264592  1.6120689\n",
      "   1.3292024  0.5010722  0.74973434 1.8131617 ]\n",
      "  [0.1764662  1.4836222  1.4041927  0.5026054  0.9945107  1.0669972\n",
      "   0.14846809 1.6964966  0.86043406 1.4357265 ]\n",
      "  [0.93734354 1.4543378  1.3062835  1.6834066  0.8725772  0.83437973\n",
      "   1.2740138  0.4986254  1.8588145  0.08290935]\n",
      "  [0.1229637  0.41337132 0.5389994  0.6307979  1.1022484  0.5786191\n",
      "   1.7612604  1.1222043  1.3118982  1.1474959 ]\n",
      "  [1.445092   0.02748177 0.6794948  1.9054545  0.6982391  0.30442306\n",
      "   0.22941129 0.02391113 1.7263323  1.1730708 ]\n",
      "  [1.4013805  0.95061255 0.93180466 1.8439083  0.9744648  0.711788\n",
      "   1.6974566  0.46015173 1.4346411  1.0096024 ]\n",
      "  [0.02604794 1.2811319  1.6362531  0.33985102 0.94325614 1.8907872\n",
      "   0.29253644 0.03019303 1.606465   1.7228177 ]\n",
      "  [1.2820742  0.3490698  0.15092024 1.8436383  1.8030896  0.83446914\n",
      "   0.5877944  0.67581123 1.1145767  0.08571426]\n",
      "  [1.7680917  1.0784062  1.8558294  1.7641828  1.1737585  0.9499003\n",
      "   0.36871007 0.6251928  0.54631686 0.6557991 ]\n",
      "  [0.36560073 0.9488855  1.611183   0.45937845 1.6762083  1.4113396\n",
      "   0.193377   0.8178718  1.7754053  0.17864338]\n",
      "  [1.1256527  0.81022143 0.6946514  0.59193575 1.5893481  1.6974437\n",
      "   1.3556553  1.3185804  0.90500224 1.0250541 ]\n",
      "  [0.171953   1.2907926  1.2992797  0.0232781  0.0619078  1.9907385\n",
      "   1.2301521  1.8972675  1.9176631  0.17391653]\n",
      "  [1.605612   1.3463904  1.3050002  0.22691156 1.1895015  1.9666827\n",
      "   1.9325516  0.2840017  0.5586242  1.597751  ]\n",
      "  [1.1007241  0.37229595 0.38239467 0.1782283  0.5605785  1.5876404\n",
      "   0.6714734  1.2004933  0.37181756 0.2637996 ]\n",
      "  [1.7527448  0.5683652  0.31263372 1.1677732  0.25685823 1.2744585\n",
      "   1.726395   0.11344469 1.0937661  1.4874107 ]\n",
      "  [1.4163206  1.7419285  1.1536485  0.02079747 0.9632655  1.6283097\n",
      "   1.5084131  0.7055885  0.48845115 0.9563711 ]\n",
      "  [0.823612   1.4612864  1.8008271  0.13373317 0.4544628  1.4675512\n",
      "   0.7089521  0.7921184  1.6112808  0.92773396]\n",
      "  [0.02237331 1.0912628  0.5420223  1.0132782  0.9909398  1.0997703\n",
      "   1.7514807  0.37598813 0.41403428 1.9110271 ]\n",
      "  [0.86125094 0.42789558 1.3032445  1.2518543  1.209709   1.1162232\n",
      "   1.7607267  1.6246283  0.22392161 1.7272353 ]\n",
      "  [0.4453654  0.9694684  1.4275972  1.5737298  0.5244337  0.06777016\n",
      "   0.3628408  0.8695007  0.01760602 0.9097001 ]\n",
      "  [0.02194559 1.5195736  1.8819368  0.39590624 0.8146555  1.3185401\n",
      "   0.5253826  0.90939695 0.2927133  1.8254263 ]\n",
      "  [1.5358074  1.0611671  0.4968662  0.36078966 0.34920007 1.6877888\n",
      "   0.39183643 1.3175952  0.6467985  1.4464735 ]]]\n",
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  7\n",
      "Action Space Sample 2\n",
      "(1, 29, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n",
    "\n",
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n",
    "\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size, h2_size):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # Convolutional Network\n",
    "        self.conv = nn.Sequential(nn.Conv2d(s_size[0], h_size, 4), nn.ReLU(),\n",
    "            nn.Conv2d(h_size, h_size, kernel_size=3, stride=1),nn.ReLU())\n",
    "        # print(self.conv(torch.zeros(1,*s_size,4)).size())\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(self.conv(torch.zeros(1,*s_size)).size())), h2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_size, a_size)\n",
    "        )\n",
    "\n",
    "        # Linear Network\n",
    "        # self.fc1 = nn.Linear(np.prod(s_size), h_size)\n",
    "        # self.fc2 = nn.Linear(h_size, h_size)\n",
    "        # self.fc3 = nn.Linear(h_size, a_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convlutional Network\n",
    "        x = self.conv(x).view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # Linear Network\n",
    "        # x = np.reshape(x,(1,-1))\n",
    "        # x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "\n",
    "        # print(np.shape(x))\n",
    "        # We output the softmax\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state) # Code Here: get the action\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "\n",
    "        ## We compute this starting from the last timestep to the first, to avoid redundant computations\n",
    "\n",
    "        ## appendleft() function of queues appends to the position 0\n",
    "        ## We use deque instead of lists to reduce the time complexity\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(gamma*disc_return_t + rewards[t]) # Code Here: complete here\n",
    "\n",
    "        ## standardization for training stability\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps is added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Line 8: PyTorch prefers gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"h2_size\": 512,\n",
    "    \"n_training_episodes\": 50000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "# Create policy and place it to the device\n",
    "torch.manual_seed(50) # Don't change this\n",
    "tetris_policy = Policy(hyperparameters[\"state_space\"], hyperparameters[\"action_space\"], hyperparameters[\"h_size\"], hyperparameters[\"h2_size\"]).to(device)\n",
    "tetris_optimizer = optim.Adam(tetris_policy.parameters(), lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading existing policy\n",
    "state = torch.load('./training_state_2023.12.07-11.04.44.data')\n",
    "tetris_optimizer.load_state_dict(state['optimizer'])\n",
    "tetris_policy.load_state_dict(state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -2.12\n",
      "Episode 200\tAverage Score: -2.52\n",
      "Episode 300\tAverage Score: -2.35\n",
      "Episode 400\tAverage Score: -2.79\n",
      "Episode 500\tAverage Score: -2.85\n",
      "Episode 600\tAverage Score: -2.28\n",
      "Episode 700\tAverage Score: -2.19\n",
      "Episode 800\tAverage Score: -2.77\n",
      "Episode 900\tAverage Score: -2.92\n",
      "Episode 1000\tAverage Score: -3.19\n",
      "Episode 1100\tAverage Score: -3.17\n",
      "Episode 1200\tAverage Score: -1.95\n",
      "Episode 1300\tAverage Score: -2.64\n",
      "Episode 1400\tAverage Score: -2.03\n",
      "Episode 1500\tAverage Score: -1.70\n",
      "Episode 1600\tAverage Score: -1.72\n",
      "Episode 1700\tAverage Score: -1.78\n",
      "Episode 1800\tAverage Score: -1.79\n",
      "Episode 1900\tAverage Score: -1.75\n",
      "Episode 2000\tAverage Score: -2.06\n",
      "Episode 2100\tAverage Score: -3.03\n",
      "Episode 2200\tAverage Score: -2.36\n",
      "Episode 2300\tAverage Score: -2.74\n",
      "Episode 2400\tAverage Score: -2.29\n",
      "Episode 2500\tAverage Score: -1.84\n",
      "Episode 2600\tAverage Score: -2.55\n",
      "Episode 2700\tAverage Score: -2.80\n",
      "Episode 2800\tAverage Score: -2.10\n",
      "Episode 2900\tAverage Score: -1.75\n",
      "Episode 3000\tAverage Score: -2.01\n",
      "Episode 3100\tAverage Score: -2.05\n",
      "Episode 3200\tAverage Score: -2.73\n",
      "Episode 3300\tAverage Score: -1.63\n",
      "Episode 3400\tAverage Score: -1.71\n",
      "Episode 3500\tAverage Score: -1.73\n",
      "Episode 3600\tAverage Score: -1.65\n",
      "Episode 3700\tAverage Score: -1.39\n",
      "Episode 3800\tAverage Score: -1.81\n",
      "Episode 3900\tAverage Score: -1.46\n",
      "Episode 4000\tAverage Score: -1.61\n",
      "Episode 4100\tAverage Score: -1.67\n",
      "Episode 4200\tAverage Score: -1.47\n",
      "Episode 4300\tAverage Score: -1.88\n",
      "Episode 4400\tAverage Score: -1.72\n",
      "Episode 4500\tAverage Score: -1.32\n",
      "Episode 4600\tAverage Score: -1.34\n",
      "Episode 4700\tAverage Score: -1.25\n",
      "Episode 4800\tAverage Score: -1.29\n",
      "Episode 4900\tAverage Score: -1.20\n",
      "Episode 5000\tAverage Score: -1.16\n",
      "Episode 5100\tAverage Score: -1.42\n",
      "Episode 5200\tAverage Score: -1.14\n",
      "Episode 5300\tAverage Score: -1.08\n",
      "Episode 5400\tAverage Score: -0.98\n",
      "Episode 5500\tAverage Score: -1.05\n",
      "Episode 5600\tAverage Score: -0.72\n",
      "Episode 5700\tAverage Score: -0.92\n",
      "Episode 5800\tAverage Score: -2.62\n",
      "Episode 5900\tAverage Score: -0.87\n",
      "Episode 6000\tAverage Score: -1.28\n",
      "Episode 6100\tAverage Score: -0.84\n",
      "Episode 6200\tAverage Score: -1.19\n",
      "Episode 6300\tAverage Score: -1.00\n",
      "Episode 6400\tAverage Score: -1.14\n",
      "Episode 6500\tAverage Score: -0.87\n",
      "Episode 6600\tAverage Score: -0.92\n",
      "Episode 6700\tAverage Score: -0.62\n",
      "Episode 6800\tAverage Score: -0.61\n",
      "Episode 6900\tAverage Score: -0.29\n",
      "Episode 7000\tAverage Score: -0.29\n",
      "Episode 7100\tAverage Score: -0.38\n",
      "Episode 7200\tAverage Score: -0.66\n",
      "Episode 7300\tAverage Score: -0.58\n",
      "Episode 7400\tAverage Score: -0.54\n",
      "Episode 7500\tAverage Score: -0.66\n",
      "Episode 7600\tAverage Score: -0.88\n",
      "Episode 7700\tAverage Score: -0.66\n",
      "Episode 7800\tAverage Score: -0.65\n",
      "Episode 7900\tAverage Score: -0.53\n",
      "Episode 8000\tAverage Score: -0.88\n",
      "Episode 8100\tAverage Score: -0.61\n",
      "Episode 8200\tAverage Score: -0.56\n",
      "Episode 8300\tAverage Score: -0.57\n",
      "Episode 8400\tAverage Score: -0.33\n",
      "Episode 8500\tAverage Score: -0.04\n",
      "Episode 8600\tAverage Score: 0.05\n",
      "Episode 8700\tAverage Score: -0.41\n",
      "Episode 8800\tAverage Score: -0.13\n",
      "Episode 8900\tAverage Score: -0.50\n",
      "Episode 9000\tAverage Score: -0.34\n",
      "Episode 9100\tAverage Score: -0.21\n",
      "Episode 9200\tAverage Score: -0.50\n",
      "Episode 9300\tAverage Score: -0.49\n",
      "Episode 9400\tAverage Score: -0.43\n",
      "Episode 9500\tAverage Score: -0.34\n",
      "Episode 9600\tAverage Score: -0.21\n",
      "Episode 9700\tAverage Score: -0.33\n",
      "Episode 9800\tAverage Score: -0.35\n",
      "Episode 9900\tAverage Score: -0.09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ai/Desktop/tetris2/tetris_gym_env/tetris conv.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     first \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     scores \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m scores\u001b[39m.\u001b[39mextend(reinforce(tetris_policy,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                    tetris_optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                    hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mn_training_episodes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                    hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mmax_t\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                    hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                    \u001b[39m100\u001b[39;49m))\n",
      "\u001b[1;32m/home/ai/Desktop/tetris2/tetris_gym_env/tetris conv.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Line 8: PyTorch prefers gradient descent\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Desktop/tetris2/tetris_gym_env/tetris%20conv.ipynb#X12sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m i_episode \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if first:\n",
    "    first = False\n",
    "    scores = []\n",
    "scores.extend(reinforce(tetris_policy,\n",
    "                   tetris_optimizer,\n",
    "                   hyperparameters[\"n_training_episodes\"],\n",
    "                   hyperparameters[\"max_t\"],\n",
    "                   hyperparameters[\"gamma\"],\n",
    "                   100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving current policy for continued training\n",
    "from time import gmtime, strftime\n",
    "curr_time = strftime(\"%Y.%m.%d-%H.%M.%S\", gmtime())\n",
    "training_state = {'model':tetris_policy.state_dict(), 'optimizer':tetris_optimizer.state_dict()}\n",
    "torch.save(training_state, f'./training_state_conv_{curr_time}.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./training_scores_200k.txt','w')\n",
    "for item in scores:\n",
    "\tfile.write(f'{item} ')\n",
    "file.write('\\n')\n",
    "file.flush()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  for frame in range(5000):\n",
    "    if done:\n",
    "      break\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1200, 660) to (1200, 672) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "curr_time = strftime(\"%Y.%m.%d-%H.%M.%S\", gmtime())\n",
    "record_video(eval_env, tetris_policy, f'./replay_conv_{curr_time}.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
