{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "# import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "# from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TRAINING STATE FILE (.data) HERE\n",
    "state = torch.load('./training_all_spinning_in_place.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Tetris v3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:20: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: float32. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:25: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_id = \"gym_tetris:tetris_rl\"\n",
    "env = gym.make(env_id, render_mode='none')\n",
    "s_size = env.observation_space.shape\n",
    "a_size = env.action_space.n\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size, h2_size):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # Convolutional Network\n",
    "        # self.conv = nn.Sequential(nn.Conv2d(s_size[0], h_size, 4), nn.ReLU(),\n",
    "        #     nn.Conv2d(h_size, h_size, kernel_size=3, stride=1),nn.ReLU())\n",
    "        # # print(self.conv(torch.zeros(1,*s_size,4)).size())\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(int(np.prod(self.conv(torch.zeros(1,*s_size)).size())), h2_size),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(h2_size, a_size)\n",
    "        # )\n",
    "\n",
    "        # Linear Network\n",
    "        self.fc1 = nn.Linear(np.prod(s_size), h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size)\n",
    "        self.fc3 = nn.Linear(h_size, a_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convlutional Network\n",
    "        # x = self.conv(x).view(x.size()[0], -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        # Linear Network\n",
    "        x = np.reshape(x,(1,-1))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # print(np.shape(x))\n",
    "        # We output the softmax\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create policy and place it to the device\n",
    "hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"h2_size\": 512,\n",
    "    \"n_training_episodes\": 50000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}\n",
    "torch.manual_seed(50) # Don't change this\n",
    "tetris_policy = Policy(hyperparameters[\"state_space\"], hyperparameters[\"action_space\"], hyperparameters[\"h_size\"], hyperparameters[\"h2_size\"]).to(device)\n",
    "tetris_optimizer = optim.Adam(tetris_policy.parameters(), lr=hyperparameters[\"lr\"])\n",
    "tetris_optimizer.load_state_dict(state['optimizer'])\n",
    "tetris_policy.load_state_dict(state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "def create_video(env, policy):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  env.render()\n",
    "  for frame in range(5000):\n",
    "    if done:\n",
    "      break\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render()\n",
    "  pygame.display.quit()\n",
    "  pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Tetris v3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:20: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: float32. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:25: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/ai/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(env_id, render_mode='human')\n",
    "create_video(eval_env, tetris_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
